\section{Our approach} %How will you solve the problem
\label{cha:approach}

\subsection{Preprocessing}
\label{sec:preprocessing}
%\subsection{Integration of JSON data}
%\label{sec:integration}
\textbf{Integration of JSON data}: As the source format of the data is JSON, %Abbreviations ?!
the dataset cannot simply be added as a RapidMiner repository but has to be transformed into a format RapidMiner can process.\\
We've considered two options: RapidMiner offers an operator "JSON to data", which takes a collection of JSON documents and transforms it into an example set with one example per document. With further transformation, including transposing columns into rows and transforming the array dimensions into separate columns using regular expressions, the JSON data can be transformed into a tabular structure. Unfortunately, the respective JSON-operator is expecting an array of JSON documents (or separate files for each object) in order to create the correct structure. As the dataset contains a large amount of objects in a one json-object per line style, this implies it would be necessary to further preprocess the provided files: Either splitting them into separate files for each JSON object (per type), or transforming the object type files into an array of all objects.\\
Thus, a second option would be to directly transform the existing JSON files into a format RapidMiner can work with, using external tools. In their github repository (\url{https://github.com/Yelp/dataset-examples}), contributers have already provided scripts that execute JSON to csv conversion. These scripts can be used as a starting point to transform the dataset into a usefule format for RapidMiner. %How?!
%Challenge size of dataset? 

- As a first step we will filter the businesses to look at during the process-building-phase. The processing steps will take too long to apply to the entire dataset several times from the start. we will begin by sampling a rather small but still representative subset of the data to work on. This dataset will contain businesses meta data and textual reviews for around 50 businesses. We will also extract the reviews of 10 businesses as our validation set and another 10 as our test set. When sampling the data we will make sure to include businesses that have positive overall ratings and also businesses with negative overall ratings to create a balanced dataset. Both the validation and test set will need manual evaluation of the sentiments for the different aspects mentioned in the reviews. Thus we will only include businesses in the validation and test set that have a manageable amount of reviews. \\
- Once we have the data sampled we will extract the reviews for each business and create a concatenated version of them in a single document. We do not need the information which sentences originated from which review since we want to look at the entire set of reviews to extract new attributes for the business.\\
- At this stage we will proceed with common text preprocessing methods. Those will include tokenization, stop-word removal and POS tagging. We will follow with our approach a method published by Bancken, Alfarone and Davis in "`Automatically Detecting and Rating Product Aspects from Textual Customer Reviews'". We will make use of the Stanford CoreNLP to apply the preprocessing steps and extract syntactical dependencies on a per sentence base. \\
- The next step will be to build an algorithm that is able to extract the relevant syntactical dependencies identified by the Stanford Parser. We will build on the relevant dependencies identified in the paper by Bancken, Alfarone and Davis. The result of this preprocessing step are tuples in the form <sentiment modifier, potential aspect>. \\
- After that we will apply a clustering algorithm (k-means or k-medoids) to cluster tuples together that express a sentiment for the same aspect. To accomplish this we will use a Word-net based similarity metric calles Jcn. An implementation of this metric can be found in the WS4J library. The similarity metric will be calculated for each pair of potential aspects after they were stemmed to reduce the number of different words. The metric will then serve as input to a clustering algorithm which will output clusters that represent different aspects. As a result of this step we can reduce the number of potential aspects to increase our precision based on the cluster-size.\\
- Afterwards we will make us of a sentiment lexicon for the English language to determine sentiment values for the sentiment modifiers identified in a previous step. As a result we will be able to assign sentiment values to aspect-clusters which were identified in the previous step. We will then extract the most positive and the most negative aspects which form our result.

\subsection{Algorithms}
\label{sec:algorithms}